{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3868e4d8-b483-4cf8-9c50-4094cb258e3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## unicode1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aacb51-9e5b-4627-a7dd-39fb2b04e55f",
   "metadata": {},
   "source": [
    "ord()函数用于获取字符的Unicode码点（整数表示）\n",
    "chr()函数用于将Unicode码点转换为对应的字符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa6254-e0a9-4b2d-8424-3dc93a2b09a9",
   "metadata": {},
   "source": [
    "chr(0) 是 空字符（Null character），在Python中表示为 '\\x00'。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8fd911-e4a3-4f44-b7e7-76d0adb98fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a54515dd-c744-4cd7-b8de-03a0022a24c4",
   "metadata": {},
   "source": [
    "repr()返回的是字符的官方表示，通常尽可能明确地表示对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9246f48c-bde1-4e02-9b58-c16f2675844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "'\\n'\n"
     ]
    }
   ],
   "source": [
    "s = '\\n'\n",
    "print(s)\n",
    "print(repr(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa933563-1985-41ce-b9cd-1356d6538929",
   "metadata": {},
   "source": [
    "chr(0)虽然表示空字符(\\0)，但并不会导致字符串结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2112901-1bf8-4ea9-915a-29e61bb368f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "chr(0)\n",
    "print(chr(0))\n",
    "\"this is a test\" + chr(0) + \"string\"\n",
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0cfd0-4e1d-4692-a103-1a3de34af46d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## unicode2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f81206-37ce-4c9b-99b8-3b207bf26f27",
   "metadata": {},
   "source": [
    "1. 效率与字节级表示\n",
    "现代Tokenizer（如BERT使用的WordPiece、GPT使用的BPE）的核心思想是在子词级别进行分割，而一个越来越流行的趋势是直接在字节级别进行子词学习。\n",
    "\n",
    "UTF-8是变长编码（1到4个字节），但它有一个关键优势：单字节编码的部分与ASCII完全兼容。这意味着所有英文字母、数字和常用符号在UTF-8中都被表示为单个字节。\n",
    "\n",
    "Tokenizer的工作方式：当我们在UTF-8上训练BPE时，我们本质上是在对字节序列进行合并操作。模型最初看到的是一个个字节，然后学习将频繁共现的字节组合成更大的token。\n",
    "\n",
    "使用UTF-8：\n",
    "\n",
    "一个英文字符（如 a）是1个字节。\n",
    "\n",
    "一个欧洲字符（如 é）是2个字节。\n",
    "\n",
    "一个常见的中文汉字（如 中）是3个字节。\n",
    "\n",
    "BPE算法可以自由地将单个字节或字节组合成token。例如，它可能会学到 \"the\" 是一个token（3个字节合并），也可能会学到 \"中\" 是一个token（3个特定字节的合并）。这非常灵活和高效。\n",
    "\n",
    "使用UTF-16：\n",
    "\n",
    "绝大多数常用字符（包括整个BMP基本多文种平面）都固定用2个字节表示。\n",
    "\n",
    "这意味着即使是英文字母 a，也会被表示为 0061（2个字节）。这会立即将你的初始词汇表大小（在BPE之前）翻倍，并且其中一半的字节（前导的00）几乎是冗余的，因为英文文本中大部分时间高字节都是0。\n",
    "\n",
    "Tokenizer需要处理这些大量的、无信息的00字节，学习效率低下。\n",
    "\n",
    "使用UTF-32：\n",
    "\n",
    "每个字符都固定用4个字节表示。\n",
    "\n",
    "这将是效率的灾难。一个简单的英文文本文件，其原始大小会膨胀为UTF-8版本的4倍。\n",
    "\n",
    "初始词汇表（在BPE之前）是巨大的4字节单元，其中对于拉丁语系文本，每个字符的前3个字节基本都是0。这对Tokenizer的学习和模型的计算都是巨大的浪费。\n",
    "\n",
    "结论1：UTF-8为Tokenizer提供了最紧凑、信息密度最高的起点（字节序列），避免了UTF-16/32中大量的空格浪费。\n",
    "\n",
    "2. 词汇表大小与模型复杂度\n",
    "Tokenizer训练的目标之一是生成一个大小固定的、高效的词汇表。\n",
    "\n",
    "UTF-8的基础原子只有256种可能（0-255的字节值）。这是一个非常小且易于管理的起点。BPE可以从这256个字节开始，稳健地构建出包含数万到数十万个token的词汇\n",
    "\n",
    "表，这些token可以有效地表示任何语言中的任何单词。\n",
    "\n",
    "3. 兼容性与实践性\n",
    "Web和数据的现实：互联网上绝大部分文本数据默认都是以UTF-8编码的。操作系统、文件处理和网络传输也广泛支持UTF-8。从数据源到模型训练，使用UTF-8意味着更少的编码解码转换，减少了出错的可能。\n",
    "\n",
    "统一处理：使用基于UTF-8的字节级Tokenizer（如SentencePiece），可以设计出一个真正的单一、统一的多语言模型。这个模型可以处理任何混合语言的文本，而无需预先知道文本的语言信息。因为所有语言最终都被分解成了相同的256个字节的序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc259ac3-e909-4440-a685-7bdd90194f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    " return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68498ca9-876a-47ec-bc97-8cab7fa22818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    " return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "decode_utf8_bytes_to_str_wrong(\"中文\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc776e-5ddb-446d-977a-47dc4a5b84a3",
   "metadata": {},
   "source": [
    "此处报错的原因是decode_utf8_bytes_to_str_wrong（）函数的功能是遍历字节串中的每个字节，将每个整数字节转为单字节bytes，然后单独对每个字节进行UTF-8解码，再将结果拼接为字符串。‘hello’能正常运行的原因是其中的每个字符都为有效的ASCII字符，在单字节解码时不会出错。而‘中文’中的每个字符都是三字节字符，转换后的字节本身不是有效的UTF-8字符，所以无法正常进行UFT-8解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e006982-f7e1-4334-87ef-7dff152bbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytestring = \"hello\".encode(\"utf-8\")\n",
    "print(bytestring)\n",
    "bytestring = \"中文\".encode(\"utf-8\")\n",
    "print(bytestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cb4fe-f1e1-4ad2-8bbd-95d489407a05",
   "metadata": {},
   "source": [
    "有效的 UTF-8 双字节序列格式为：110xxxxx 10xxxxxx\n",
    "无效的序列示例：0xC1 0xBF（无效起始字节）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a843a0b-37d3-4f39-825f-12658012fb6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## train_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda659eb-ce9d-4ba3-a444-066df85f02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: List[str], max_merges: int = None) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    训练字节级BPE分词器\n",
    "    \n",
    "    Args:\n",
    "        input_path: 训练数据文本文件的路径\n",
    "        vocab_size: 最终词汇表大小（包括初始字节词汇、合并产生的词汇和特殊令牌）\n",
    "        special_tokens: 要添加到词汇表中的特殊令牌列表\n",
    "        max_merges: 最大合并次数，如果为None则不限制\n",
    "    \n",
    "    Returns:\n",
    "        vocab: 词汇表，映射从token ID到bytes\n",
    "        merges: BPE合并操作列表，按创建顺序排列\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 初始化词汇表（256个字节 + 特殊令牌）\n",
    "    vocab = {}\n",
    "    next_id = 0\n",
    "    \n",
    "    # 添加基础字节词汇 (0-255)\n",
    "    for i in range(256):\n",
    "        vocab[next_id] = bytes([i])\n",
    "        next_id += 1\n",
    "    \n",
    "    # 添加特殊令牌\n",
    "    special_token_bytes = []\n",
    "    for token in special_tokens:\n",
    "        token_bytes = token.encode('utf-8')\n",
    "        special_token_bytes.append(token_bytes)\n",
    "        vocab[next_id] = token_bytes\n",
    "        next_id += 1\n",
    "    \n",
    "    # 2. 读取训练数据并统计词频\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # 将文本转换为UTF-8字节序列\n",
    "    byte_data = text.encode('utf-8')\n",
    "    \n",
    "    # 3. 初始化词汇：将文本拆分为单词，每个单词拆分为字节\n",
    "    words = text.split()\n",
    "    word_freqs = collections.Counter(words)\n",
    "    \n",
    "    # 构建初始词汇统计（字节序列）\n",
    "    vocab_stats = {}\n",
    "    for word, freq in word_freqs.items():\n",
    "        byte_word = word.encode('utf-8')\n",
    "        # 将单词表示为字节列表\n",
    "        tokens = [bytes([b]) for b in byte_word]\n",
    "        vocab_stats[tuple(tokens)] = freq\n",
    "    \n",
    "    # 4. BPE训练循环\n",
    "    merges = []\n",
    "    merge_count = 0  # 添加合并次数计数器\n",
    "    \n",
    "    while len(vocab) < vocab_size:\n",
    "        # 检查是否达到最大合并次数限制\n",
    "        if max_merges is not None and merge_count >= max_merges:\n",
    "            break\n",
    "            \n",
    "        # 统计所有相邻字节对的出现频率\n",
    "        pair_freqs = collections.Counter()\n",
    "        \n",
    "        for tokens, freq in vocab_stats.items():\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        \n",
    "        if not pair_freqs:\n",
    "            break  # 没有更多可以合并的对\n",
    "        \n",
    "        # 找到频率最高的字节对（如果有多个相同频率的，选择最后一个）\n",
    "        max_freq = max(pair_freqs.values())\n",
    "        # 找到所有频率等于最大频率的字节对\n",
    "        best_pairs = [pair for pair, freq in pair_freqs.items() if freq == max_freq]\n",
    "        # 选择最后一个字节对\n",
    "        best_pair = best_pairs[-1] if best_pairs else None\n",
    "        \n",
    "        # 检查是否达到词汇表大小限制\n",
    "        if len(vocab) >= vocab_size:\n",
    "            break\n",
    "        \n",
    "        # 创建新的合并token\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "        \n",
    "        # 添加到词汇表\n",
    "        vocab[next_id] = new_token\n",
    "        next_id += 1\n",
    "        \n",
    "        # 记录合并操作\n",
    "        merges.append(best_pair)\n",
    "        merge_count += 1  # 增加合并计数器\n",
    "        \n",
    "        # 更新词汇统计：合并所有出现的best_pair\n",
    "        new_vocab_stats = {}\n",
    "        for tokens, freq in vocab_stats.items():\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if (i < len(tokens) - 1 and \n",
    "                    tokens[i] == best_pair[0] and \n",
    "                    tokens[i + 1] == best_pair[1]):\n",
    "                    new_tokens.append(new_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            new_vocab_stats[tuple(new_tokens)] = freq\n",
    "        \n",
    "        vocab_stats = new_vocab_stats\n",
    "    \n",
    "    return vocab, merges\n",
    "\n",
    "# 测试函数\n",
    "def test_bpe():\n",
    "    \"\"\"测试BPE训练函数\"\"\"\n",
    "    \n",
    "    user_dir = os.path.expanduser('~')\n",
    "    file_path = os.path.join(user_dir, 'Downloads', 'tinystories_sample.txt')\n",
    "    \n",
    "    # 训练BPE分词器，限制最多合并6次\n",
    "    vocab, merges = train_bpe(\n",
    "        input_path='file_path',\n",
    "        vocab_size=,\n",
    "        special_tokens=['<pad>', '<unk>', '<s>', '</s>'],\n",
    "        max_merges=6  # 添加合并次数限制\n",
    "    )\n",
    "    \n",
    "    print(\"词汇表大小:\", len(vocab))\n",
    "    print(\"实际合并次数:\", len(merges))\n",
    "    print(\"\\n前20个词汇项:\")\n",
    "    for i, (token_id, token_bytes) in enumerate(list(vocab.items())[:20]):\n",
    "        print(f\"ID {token_id}: {token_bytes} -> {token_bytes.decode('utf-8', errors='replace')}\")\n",
    "    \n",
    "    print(f\"\\n特殊令牌:\")\n",
    "    for token in ['<pad>', '<unk>', '<s>', '</s>']:\n",
    "        token_bytes = token.encode('utf-8')\n",
    "        for token_id, bytes_val in vocab.items():\n",
    "            if bytes_val == token_bytes:\n",
    "                print(f\"ID {token_id}: {token_bytes} -> {token}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n合并操作数量: {len(merges)}\")\n",
    "    if merges:\n",
    "        print(\"合并操作列表:\")\n",
    "        for i, merge in enumerate(merges):\n",
    "            print(f\"  {i+1}: {merge[0]} + {merge[1]} -> {merge[0] + merge[1]}\")\n",
    "    print(vocab)\n",
    "    print('/n')\n",
    "    print('/n')\n",
    "    print('/n')\n",
    "    print(merges)\n",
    "    # 清理测试文件\n",
    "    import os\n",
    "    if os.path.exists('test_bpe.txt'):\n",
    "        os.remove('test_bpe.txt')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_bpe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96961b75-356b-4cce-ab37-0bf641d63bca",
   "metadata": {},
   "source": [
    "组合："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d037787-5f5d-4438-be97-2a7e3b4fba94",
   "metadata": {},
   "source": [
    "1.创建主函数和导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233f5a4-5ccc-43aa-ad92-75c085f87469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import multiprocessing as mp\n",
    "from typing import Dict, List, Tuple, BinaryIO\n",
    "import heapq\n",
    "\n",
    "def parallel_bpe_training(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str],\n",
    "    max_merges: int = None,\n",
    "    num_processes: int = 4,\n",
    "    chunk_token: bytes = b\"<|endoftext|>\"\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    并行BPE训练主函数\n",
    "    \n",
    "    Args:\n",
    "        input_path: 输入文件路径\n",
    "        vocab_size: 目标词汇表大小\n",
    "        special_tokens: 特殊标记列表\n",
    "        max_merges: 最大合并次数\n",
    "        num_processes: 进程数\n",
    "        chunk_token: 分块标记\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 分块处理\n",
    "    chunk_results = process_chunks_parallel(\n",
    "        input_path, num_processes, chunk_token, special_tokens\n",
    "    )\n",
    "    \n",
    "    # 2. 合并统计结果\n",
    "    global_word_freqs = merge_chunk_statistics(chunk_results)\n",
    "    \n",
    "    # 3. 全局BPE训练\n",
    "    vocab, merges = train_bpe_from_statistics(\n",
    "        global_word_freqs, vocab_size, special_tokens, max_merges\n",
    "    )\n",
    "    \n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe1ed5-c672-43cc-80f7-2f3038c9a6d5",
   "metadata": {},
   "source": [
    "2.并行处理分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c82c2-05db-46cb-9b67-f4387c248575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(args):\n",
    "    \"\"\"处理单个分块的worker函数\"\"\"\n",
    "    start, end, input_path, chunk_token, special_tokens = args\n",
    "    \n",
    "    word_freqs = collections.Counter()\n",
    "    \n",
    "    with open(input_path, 'rb') as f:\n",
    "        f.seek(start)\n",
    "        chunk_data = f.read(end - start)\n",
    "        \n",
    "        try:\n",
    "            # 解码并处理分块\n",
    "            text = chunk_data.decode('utf-8', errors='ignore')\n",
    "            words = text.split()\n",
    "            \n",
    "            # 统计词频（转换为字节序列）\n",
    "            for word in words:\n",
    "                byte_word = word.encode('utf-8')\n",
    "                word_freqs[byte_word] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"处理分块 [{start}-{end}] 时出错: {e}\")\n",
    "    \n",
    "    return dict(word_freqs)\n",
    "\n",
    "def process_chunks_parallel(input_path, num_processes, chunk_token, special_tokens):\n",
    "    \"\"\"并行处理所有分块\"\"\"\n",
    "    \n",
    "    # 获取分块边界\n",
    "    with open(input_path, 'rb') as f:\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, chunk_token)\n",
    "    \n",
    "    # 准备参数\n",
    "    chunk_args = []\n",
    "    for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "        chunk_args.append((start, end, input_path, chunk_token, special_tokens))\n",
    "    \n",
    "    # 并行处理\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(process_chunk, chunk_args)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe356290-2e0f-47b0-9727-d6b4ed5c1d55",
   "metadata": {},
   "source": [
    "3. 合并统计结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71726d-1e36-4d44-afe3-b902302c69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chunk_statistics(chunk_results):\n",
    "    \"\"\"合并所有分块的统计结果\"\"\"\n",
    "    global_word_freqs = collections.Counter()\n",
    "    \n",
    "    for chunk_freqs in chunk_results:\n",
    "        for word_bytes, freq in chunk_freqs.items():\n",
    "            global_word_freqs[word_bytes] += freq\n",
    "    \n",
    "    return global_word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba2d3b-63ae-45f2-8078-2eb7e6beda9f",
   "metadata": {},
   "source": [
    "4. 基于统计的BPE训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c4d80-a2c9-4c2c-ba02-40880cd96c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_from_statistics(\n",
    "    word_freqs: Dict[bytes, int],\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str],\n",
    "    max_merges: int = None\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    基于词频统计训练BPE\n",
    "    \"\"\"\n",
    "    \n",
    "    # 初始化词汇表\n",
    "    vocab = {}\n",
    "    next_id = 0\n",
    "    \n",
    "    # 添加基础字节词汇\n",
    "    for i in range(256):\n",
    "        vocab[next_id] = bytes([i])\n",
    "        next_id += 1\n",
    "    \n",
    "    # 添加特殊令牌\n",
    "    special_token_bytes = []\n",
    "    for token in special_tokens:\n",
    "        token_bytes = token.encode('utf-8')\n",
    "        special_token_bytes.append(token_bytes)\n",
    "        vocab[next_id] = token_bytes\n",
    "        next_id += 1\n",
    "    \n",
    "    # 构建初始词汇统计\n",
    "    vocab_stats = {}\n",
    "    for word_bytes, freq in word_freqs.items():\n",
    "        tokens = [bytes([b]) for b in word_bytes]\n",
    "        vocab_stats[tuple(tokens)] = freq\n",
    "    \n",
    "    # BPE训练循环\n",
    "    merges = []\n",
    "    merge_count = 0\n",
    "    \n",
    "    while len(vocab) < vocab_size:\n",
    "        if max_merges is not None and merge_count >= max_merges:\n",
    "            break\n",
    "            \n",
    "        # 统计字节对频率\n",
    "        pair_freqs = collections.Counter()\n",
    "        \n",
    "        for tokens, freq in vocab_stats.items():\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        \n",
    "        if not pair_freqs:\n",
    "            break\n",
    "        \n",
    "        # 找到最频繁的字节对\n",
    "        max_freq = max(pair_freqs.values())\n",
    "        best_pairs = [pair for pair, freq in pair_freqs.items() if freq == max_freq]\n",
    "        best_pair = best_pairs[-1] if best_pairs else None\n",
    "        \n",
    "        if len(vocab) >= vocab_size:\n",
    "            break\n",
    "        \n",
    "        # 创建新token\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "        vocab[next_id] = new_token\n",
    "        next_id += 1\n",
    "        \n",
    "        # 记录合并\n",
    "        merges.append(best_pair)\n",
    "        merge_count += 1\n",
    "        \n",
    "        # 更新词汇统计\n",
    "        new_vocab_stats = {}\n",
    "        for tokens, freq in vocab_stats.items():\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if (i < len(tokens) - 1 and \n",
    "                    tokens[i] == best_pair[0] and \n",
    "                    tokens[i + 1] == best_pair[1]):\n",
    "                    new_tokens.append(new_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            new_vocab_stats[tuple(new_tokens)] = freq\n",
    "        \n",
    "        vocab_stats = new_vocab_stats\n",
    "    \n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab1a7f5-8936-45bc-92f1-3f5158595915",
   "metadata": {},
   "source": [
    "5. 完整的组合代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ca172-ae9c-4067-bbb6-3b5a715c6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import multiprocessing as mp\n",
    "from typing import Dict, List, Tuple, BinaryIO\n",
    "\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "        file: BinaryIO,  # 二进制文件对象\n",
    "        desired_num_chunks: int,  # 期望的块数\n",
    "        split_special_token: bytes,  # 分割标记（字节格式）\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()  # 获取文件总大小\n",
    "    file.seek(0)  # 重置文件指针\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks  # 计算理论块大小\n",
    "\n",
    "    # 创建初始的均匀分块边界\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size  # 确保最后一个边界是文件末尾\n",
    "\n",
    "    mini_chunk_size = 4096  # 每次读取4KB进行搜索\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):  # 调整中间边界\n",
    "        initial_position = chunk_boundaries[bi]  # 初始边界位置\n",
    "        file.seek(initial_position)\n",
    "\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # 读取小块\n",
    "\n",
    "            if mini_chunk == b\"\":  # 到达文件末尾\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            found_at = mini_chunk.find(split_special_token)  # 查找分割标记\n",
    "            if found_at != -1:  # 找到标记\n",
    "                chunk_boundaries[bi] = initial_position + found_at + len(split_special_token)\n",
    "                break\n",
    "\n",
    "            initial_position += mini_chunk_size  # 继续向前搜索\n",
    "\n",
    "    return sorted(set(chunk_boundaries))  # 去重并排序\n",
    "\n",
    "\n",
    "def process_chunk(args):\n",
    "    \"\"\"处理单个分块的worker函数\"\"\"\n",
    "    start, end, input_path, chunk_token, special_tokens = args\n",
    "\n",
    "    word_freqs = collections.Counter()\n",
    "\n",
    "    with open(input_path, 'rb') as f:\n",
    "        f.seek(start)\n",
    "        chunk_data = f.read(end - start)\n",
    "\n",
    "        try:\n",
    "            # 解码并处理分块\n",
    "            text = chunk_data.decode('utf-8', errors='ignore')\n",
    "            words = text.split()\n",
    "\n",
    "            # 统计词频（转换为字节序列）\n",
    "            for word in words:\n",
    "                byte_word = word.encode('utf-8')\n",
    "                word_freqs[byte_word] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"处理分块 [{start}-{end}] 时出错: {e}\")\n",
    "\n",
    "    return dict(word_freqs)\n",
    "\n",
    "\n",
    "def process_chunks_parallel(input_path, num_processes, chunk_token, special_tokens):\n",
    "    \"\"\"并行处理所有分块\"\"\"\n",
    "\n",
    "    # 获取分块边界\n",
    "    with open(input_path, 'rb') as f:\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, chunk_token)\n",
    "\n",
    "    print(f\"分块边界: {boundaries}\")\n",
    "\n",
    "    # 准备参数\n",
    "    chunk_args = []\n",
    "    for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "        chunk_args.append((start, end, input_path, chunk_token, special_tokens))\n",
    "\n",
    "    print(f\"创建了 {len(chunk_args)} 个分块任务\")\n",
    "\n",
    "    # 并行处理\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(process_chunk, chunk_args)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def merge_chunk_statistics(chunk_results):\n",
    "    \"\"\"合并所有分块的统计结果\"\"\"\n",
    "    global_word_freqs = collections.Counter()\n",
    "\n",
    "    for chunk_freqs in chunk_results:\n",
    "        for word_bytes, freq in chunk_freqs.items():\n",
    "            global_word_freqs[word_bytes] += freq\n",
    "\n",
    "    return global_word_freqs\n",
    "\n",
    "\n",
    "def train_bpe_from_statistics(\n",
    "        word_freqs: Dict[bytes, int],\n",
    "        vocab_size: int,\n",
    "        special_tokens: List[str],\n",
    "        max_merges: int = None\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    基于词频统计训练BPE\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化词汇表\n",
    "    vocab = {}\n",
    "    next_id = 0\n",
    "\n",
    "    # 添加基础字节词汇\n",
    "    for i in range(256):\n",
    "        vocab[next_id] = bytes([i])\n",
    "        next_id += 1\n",
    "\n",
    "    # 添加特殊令牌\n",
    "    special_token_bytes = []\n",
    "    for token in special_tokens:\n",
    "        token_bytes = token.encode('utf-8')\n",
    "        special_token_bytes.append(token_bytes)\n",
    "        vocab[next_id] = token_bytes\n",
    "        next_id += 1\n",
    "\n",
    "    # 构建初始词汇统计\n",
    "    vocab_stats = {}\n",
    "    for word_bytes, freq in word_freqs.items():\n",
    "        tokens = [bytes([b]) for b in word_bytes]\n",
    "        vocab_stats[tuple(tokens)] = freq\n",
    "\n",
    "    # BPE训练循环\n",
    "    merges = []\n",
    "    merge_count = 0\n",
    "\n",
    "    while len(vocab) < vocab_size:\n",
    "        if max_merges is not None and merge_count >= max_merges:\n",
    "            break\n",
    "\n",
    "        # 统计字节对频率\n",
    "        pair_freqs = collections.Counter()\n",
    "\n",
    "        for tokens, freq in vocab_stats.items():\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "\n",
    "        if not pair_freqs:\n",
    "            break\n",
    "\n",
    "        # 找到最频繁的字节对\n",
    "        max_freq = max(pair_freqs.values())\n",
    "        best_pairs = [pair for pair, freq in pair_freqs.items() if freq == max_freq]\n",
    "        best_pair = best_pairs[-1] if best_pairs else None\n",
    "\n",
    "        if len(vocab) >= vocab_size:\n",
    "            break\n",
    "\n",
    "        # 创建新token\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "        vocab[next_id] = new_token\n",
    "        next_id += 1\n",
    "\n",
    "        # 记录合并\n",
    "        merges.append(best_pair)\n",
    "        merge_count += 1\n",
    "\n",
    "        # 更新词汇统计\n",
    "        new_vocab_stats = {}\n",
    "        for tokens, freq in vocab_stats.items():\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if (i < len(tokens) - 1 and\n",
    "                        tokens[i] == best_pair[0] and\n",
    "                        tokens[i + 1] == best_pair[1]):\n",
    "                    new_tokens.append(new_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            new_vocab_stats[tuple(new_tokens)] = freq\n",
    "\n",
    "        vocab_stats = new_vocab_stats\n",
    "\n",
    "    return vocab, merges\n",
    "\n",
    "\n",
    "def parallel_bpe_training(\n",
    "        input_path: str,\n",
    "        vocab_size: int,\n",
    "        special_tokens: List[str],\n",
    "        max_merges: int = None,\n",
    "        num_processes: int = 4,\n",
    "        chunk_token: bytes = b\"<|endoftext|>\"\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    完整的并行BPE训练流程\n",
    "    \"\"\"\n",
    "    print(\"开始并行BPE训练...\")\n",
    "    print(f\"文件: {input_path}\")\n",
    "    print(f\"进程数: {num_processes}\")\n",
    "\n",
    "    # 1. 并行分块处理\n",
    "    print(\"阶段1: 并行分块处理...\")\n",
    "    chunk_results = process_chunks_parallel(\n",
    "        input_path, num_processes, chunk_token, special_tokens\n",
    "    )\n",
    "\n",
    "    # 2. 合并统计\n",
    "    print(\"阶段2: 合并统计结果...\")\n",
    "    global_word_freqs = merge_chunk_statistics(chunk_results)\n",
    "    print(f\"统计了 {len(global_word_freqs)} 个唯一单词\")\n",
    "\n",
    "    # 3. BPE训练\n",
    "    print(\"阶段3: BPE训练...\")\n",
    "    vocab, merges = train_bpe_from_statistics(\n",
    "        global_word_freqs, vocab_size, special_tokens, max_merges\n",
    "    )\n",
    "\n",
    "    print(f\"训练完成! 词汇表大小: {len(vocab)}, 合并操作: {len(merges)}\")\n",
    "    return vocab, merges\n",
    "\n",
    "\n",
    "def test_parallel_bpe():\n",
    "    \"\"\"测试并行BPE训练\"\"\"\n",
    "    user_dir = os.path.expanduser('~')\n",
    "    file_path = os.path.join(user_dir, 'Downloads', 'tinystories_sample.txt')\n",
    "\n",
    "    # 检查文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"测试文件大小: {file_size} 字节\")\n",
    "\n",
    "    # 并行训练\n",
    "    vocab, merges = parallel_bpe_training(\n",
    "        input_path=file_path,\n",
    "        vocab_size=10000,\n",
    "        special_tokens=['<pad>', '<unk>', '<s>', '</s>'],\n",
    "        max_merges= None,\n",
    "        num_processes=2\n",
    "    )\n",
    "\n",
    "    print(\"\\n训练结果:\")\n",
    "    print(f\"词汇表大小: {len(vocab)}\")\n",
    "    print(f\"合并操作数: {len(merges)}\")\n",
    "    print(merges)\n",
    "    print(vocab)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 注意：在Windows上使用multiprocessing时需要这个保护\n",
    "    test_parallel_bpe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6cb4a5-c012-4b8f-b223-e8a8b5ff132d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## implemented tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8938aef-392a-4038-9704-bc903b028a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Dict, List, Tuple, Iterable, Iterator, Optional,BinaryIO\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "        file: BinaryIO,  # 二进制文件对象\n",
    "        desired_num_chunks: int,  # 期望的块数\n",
    "        split_special_token: bytes,  # 分割标记（字节格式）\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()  # 获取文件总大小\n",
    "    file.seek(0)  # 重置文件指针\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks  # 计算理论块大小\n",
    "\n",
    "    # 创建初始的均匀分块边界\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size  # 确保最后一个边界是文件末尾\n",
    "\n",
    "    mini_chunk_size = 4096  # 每次读取4KB进行搜索\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):  # 调整中间边界\n",
    "        initial_position = chunk_boundaries[bi]  # 初始边界位置\n",
    "        file.seek(initial_position)\n",
    "\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # 读取小块\n",
    "\n",
    "            if mini_chunk == b\"\":  # 到达文件末尾\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            found_at = mini_chunk.find(split_special_token)  # 查找分割标记\n",
    "            if found_at != -1:  # 找到标记\n",
    "                chunk_boundaries[bi] = initial_position + found_at + len(split_special_token)\n",
    "                break\n",
    "\n",
    "            initial_position += mini_chunk_size  # 继续向前搜索\n",
    "\n",
    "    return sorted(set(chunk_boundaries))  # 去重并排序\n",
    "\n",
    "\n",
    "def process_chunk(args):\n",
    "    \"\"\"处理单个分块的worker函数\"\"\"\n",
    "    start, end, input_path, chunk_token, special_tokens = args\n",
    "\n",
    "    word_freqs = collections.Counter()\n",
    "\n",
    "    with open(input_path, 'rb') as f:\n",
    "        f.seek(start)\n",
    "        chunk_data = f.read(end - start)\n",
    "\n",
    "        try:\n",
    "            # 解码并处理分块\n",
    "            text = chunk_data.decode('utf-8', errors='ignore')\n",
    "            words = text.split()\n",
    "\n",
    "            # 统计词频（转换为字节序列）\n",
    "            for word in words:\n",
    "                byte_word = word.encode('utf-8')\n",
    "                word_freqs[byte_word] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"处理分块 [{start}-{end}] 时出错: {e}\")\n",
    "\n",
    "    return dict(word_freqs)\n",
    "\n",
    "\n",
    "def process_chunks_parallel(input_path, num_processes, chunk_token, special_tokens):\n",
    "    \"\"\"并行处理所有分块\"\"\"\n",
    "\n",
    "    # 获取分块边界\n",
    "    with open(input_path, 'rb') as f:\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, chunk_token)\n",
    "\n",
    "    print(f\"分块边界: {boundaries}\")\n",
    "\n",
    "    # 准备参数\n",
    "    chunk_args = []\n",
    "    for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "        chunk_args.append((start, end, input_path, chunk_token, special_tokens))\n",
    "\n",
    "    print(f\"创建了 {len(chunk_args)} 个分块任务\")\n",
    "\n",
    "    # 并行处理\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(process_chunk, chunk_args)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def merge_chunk_statistics(chunk_results):\n",
    "    \"\"\"合并所有分块的统计结果\"\"\"\n",
    "    global_word_freqs = collections.Counter()\n",
    "\n",
    "    for chunk_freqs in chunk_results:\n",
    "        for word_bytes, freq in chunk_freqs.items():\n",
    "            global_word_freqs[word_bytes] += freq\n",
    "\n",
    "    return global_word_freqs\n",
    "\n",
    "\n",
    "def train_bpe_from_statistics(\n",
    "        word_freqs: Dict[bytes, int],\n",
    "        vocab_size: int,\n",
    "        special_tokens: List[str],\n",
    "        max_merges: int = None\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    基于词频统计训练BPE\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化词汇表\n",
    "    vocab = {}\n",
    "    next_id = 0\n",
    "\n",
    "    # 添加基础字节词汇\n",
    "    for i in range(256):\n",
    "        vocab[next_id] = bytes([i])\n",
    "        next_id += 1\n",
    "\n",
    "    # 添加特殊令牌\n",
    "    special_token_bytes = []\n",
    "    for token in special_tokens:\n",
    "        token_bytes = token.encode('utf-8')\n",
    "        special_token_bytes.append(token_bytes)\n",
    "        vocab[next_id] = token_bytes\n",
    "        next_id += 1\n",
    "\n",
    "    # 构建初始词汇统计\n",
    "    vocab_stats = {}\n",
    "    for word_bytes, freq in word_freqs.items():\n",
    "        tokens = [bytes([b]) for b in word_bytes]\n",
    "        vocab_stats[tuple(tokens)] = freq\n",
    "\n",
    "    # BPE训练循环\n",
    "    merges = []\n",
    "    merge_count = 0\n",
    "\n",
    "    while len(vocab) < vocab_size:\n",
    "        if max_merges is not None and merge_count >= max_merges:\n",
    "            break\n",
    "\n",
    "        # 统计字节对频率\n",
    "        pair_freqs = collections.Counter()\n",
    "\n",
    "        for tokens, freq in vocab_stats.items():\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "\n",
    "        if not pair_freqs:\n",
    "            break\n",
    "\n",
    "        # 找到最频繁的字节对\n",
    "        max_freq = max(pair_freqs.values())\n",
    "        best_pairs = [pair for pair, freq in pair_freqs.items() if freq == max_freq]\n",
    "        best_pair = best_pairs[-1] if best_pairs else None\n",
    "\n",
    "        if len(vocab) >= vocab_size:\n",
    "            break\n",
    "\n",
    "        # 创建新token\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "        vocab[next_id] = new_token\n",
    "        next_id += 1\n",
    "\n",
    "        # 记录合并\n",
    "        merges.append(best_pair)\n",
    "        merge_count += 1\n",
    "\n",
    "        # 更新词汇统计\n",
    "        new_vocab_stats = {}\n",
    "        for tokens, freq in vocab_stats.items():\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if (i < len(tokens) - 1 and\n",
    "                        tokens[i] == best_pair[0] and\n",
    "                        tokens[i + 1] == best_pair[1]):\n",
    "                    new_tokens.append(new_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            new_vocab_stats[tuple(new_tokens)] = freq\n",
    "\n",
    "        vocab_stats = new_vocab_stats\n",
    "\n",
    "    return vocab, merges\n",
    "\n",
    "\n",
    "def parallel_bpe_training(\n",
    "        input_path: str,\n",
    "        vocab_size: int,\n",
    "        special_tokens: List[str],\n",
    "        max_merges: int = None,\n",
    "        num_processes: int = 4,\n",
    "        chunk_token: bytes = b\"<|endoftext|>\"\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    完整的并行BPE训练流程\n",
    "    \"\"\"\n",
    "    print(\"开始并行BPE训练...\")\n",
    "    print(f\"文件: {input_path}\")\n",
    "    print(f\"进程数: {num_processes}\")\n",
    "\n",
    "    # 1. 并行分块处理\n",
    "    print(\"阶段1: 并行分块处理...\")\n",
    "    chunk_results = process_chunks_parallel(\n",
    "        input_path, num_processes, chunk_token, special_tokens\n",
    "    )\n",
    "\n",
    "    # 2. 合并统计\n",
    "    print(\"阶段2: 合并统计结果...\")\n",
    "    global_word_freqs = merge_chunk_statistics(chunk_results)\n",
    "    print(f\"统计了 {len(global_word_freqs)} 个唯一单词\")\n",
    "\n",
    "    # 3. BPE训练\n",
    "    print(\"阶段3: BPE训练...\")\n",
    "    vocab, merges = train_bpe_from_statistics(\n",
    "        global_word_freqs, vocab_size, special_tokens, max_merges\n",
    "    )\n",
    "\n",
    "    print(f\"训练完成! 词汇表大小: {len(vocab)}, 合并操作: {len(merges)}\")\n",
    "    return vocab, merges\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"BPE分词器实现\"\"\"\n",
    "\n",
    "    def __init__(self, vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]],\n",
    "                 special_tokens: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        从给定的词汇表、合并列表和特殊令牌构造分词器\n",
    "\n",
    "        Args:\n",
    "            vocab: 词汇表，映射token ID到bytes\n",
    "            merges: BPE合并操作列表\n",
    "            special_tokens: 特殊令牌列表\n",
    "        \"\"\"\n",
    "        self.vocab = vocab.copy()  # 创建副本以避免修改原始词汇表\n",
    "        self.merges = merges.copy()\n",
    "\n",
    "        # 构建反向词汇表（bytes到ID的映射）\n",
    "        self.vocab_inv = {token: idx for idx, token in self.vocab.items()}\n",
    "\n",
    "        # 处理特殊令牌\n",
    "        self.special_tokens = special_tokens or []\n",
    "        self.special_token_ids = {}\n",
    "\n",
    "        # 添加特殊令牌到词汇表（如果不存在）\n",
    "        for token in self.special_tokens:\n",
    "            token_bytes = token.encode('utf-8')\n",
    "            if token_bytes not in self.vocab_inv:\n",
    "                # 分配新的ID\n",
    "                new_id = max(self.vocab.keys()) + 1\n",
    "                self.vocab[new_id] = token_bytes\n",
    "                self.vocab_inv[token_bytes] = new_id\n",
    "            self.special_token_ids[token] = self.vocab_inv[token_bytes]\n",
    "\n",
    "        # 构建合并优先级字典\n",
    "        self.merge_priority = {}\n",
    "        for i, (a, b) in enumerate(self.merges):\n",
    "            self.merge_priority[(a, b)] = i\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        从文件加载词汇表和合并列表构造分词器\n",
    "\n",
    "        Args:\n",
    "            vocab_filepath: 词汇表文件路径\n",
    "            merges_filepath: 合并列表文件路径\n",
    "            special_tokens: 特殊令牌列表\n",
    "        \"\"\"\n",
    "        vocab = {}\n",
    "        merges = []\n",
    "\n",
    "        # 加载词汇表\n",
    "        with open(vocab_filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    token_id = int(parts[0])\n",
    "                    # 处理字节表示（可能是十六进制或原始字节）\n",
    "                    byte_repr = parts[1]\n",
    "                    if byte_repr.startswith('b\"') and byte_repr.endswith('\"'):\n",
    "                        # 处理b\"...\"格式\n",
    "                        byte_repr = byte_repr[2:-1].encode('utf-8').decode('unicode_escape').encode('latin1')\n",
    "                    elif byte_repr.startswith(\"b'\") and byte_repr.endswith(\"'\"):\n",
    "                        # 处理b'...'格式\n",
    "                        byte_repr = byte_repr[2:-1].encode('utf-8').decode('unicode_escape').encode('latin1')\n",
    "                    else:\n",
    "                        # 假设是十六进制表示\n",
    "                        try:\n",
    "                            byte_repr = bytes.fromhex(byte_repr)\n",
    "                        except ValueError:\n",
    "                            # 如果十六进制解析失败，假设是原始文本\n",
    "                            byte_repr = byte_repr.encode('utf-8')\n",
    "                    vocab[token_id] = byte_repr\n",
    "\n",
    "        # 加载合并列表\n",
    "        with open(merges_filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    # 处理字节表示\n",
    "                    a_repr = parts[0]\n",
    "                    b_repr = parts[1]\n",
    "\n",
    "                    if a_repr.startswith('b\"') and a_repr.endswith('\"'):\n",
    "                        a_bytes = a_repr[2:-1].encode('utf-8').decode('unicode_escape').encode('latin1')\n",
    "                    elif a_repr.startswith(\"b'\") and a_repr.endswith(\"'\"):\n",
    "                        a_bytes = a_repr[2:-1].encode('utf-8').decode('unicode_escape').encode('latin1')\n",
    "                    else:\n",
    "                        try:\n",
    "                            a_bytes = bytes.fromhex(a_repr)\n",
    "                        except ValueError:\n",
    "                            a_bytes = a_repr.encode('utf-8')\n",
    "\n",
    "                    if b_repr.startswith('b\"') and b_repr.endswith('\"'):\n",
    "                        b_bytes = b_repr[2:-1].encode('utf-8').decode('unicode_escape').encode('latin1')\n",
    "                    elif b_repr.startswith(\"b'\") and b_repr.endswith(\"'\"):\n",
    "                        b_bytes = b_repr[2:-1].encode('utf-8').decode('unicode_escape').encode('latin1')\n",
    "                    else:\n",
    "                        try:\n",
    "                            b_bytes = bytes.fromhex(b_repr)\n",
    "                        except ValueError:\n",
    "                            b_bytes = b_repr.encode('utf-8')\n",
    "\n",
    "                    merges.append((a_bytes, b_bytes))\n",
    "\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        将输入文本编码为token ID序列\n",
    "\n",
    "        Args:\n",
    "            text: 输入文本\n",
    "\n",
    "        Returns:\n",
    "            token ID列表\n",
    "        \"\"\"\n",
    "        # 将文本转换为字节\n",
    "        byte_sequence = text.encode('utf-8')\n",
    "\n",
    "        # 初始tokenization：将字节序列拆分为单个字节\n",
    "        tokens = [bytes([b]) for b in byte_sequence]\n",
    "\n",
    "        # 应用BPE合并\n",
    "        tokens = self._apply_merges(tokens)\n",
    "\n",
    "        # 转换为ID\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab_inv:\n",
    "                token_ids.append(self.vocab_inv[token])\n",
    "            else:\n",
    "                # 处理未知token（使用最低ID的特殊token或字节回退）\n",
    "                if self.special_tokens and '<unk>' in self.special_token_ids:\n",
    "                    token_ids.append(self.special_token_ids['<unk>'])\n",
    "                else:\n",
    "                    # 回退到单个字节编码\n",
    "                    for b in token:\n",
    "                        byte_token = bytes([b])\n",
    "                        token_ids.append(self.vocab_inv[byte_token])\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def _apply_merges(self, tokens: List[bytes]) -> List[bytes]:\n",
    "        \"\"\"应用BPE合并规则\"\"\"\n",
    "        if not self.merges:\n",
    "            return tokens\n",
    "\n",
    "        # 按照合并优先级排序\n",
    "        sorted_merges = sorted(self.merges, key=lambda x: self.merge_priority.get(x, 0))\n",
    "\n",
    "        changed = True\n",
    "        while changed and len(tokens) > 1:\n",
    "            changed = False\n",
    "            i = 0\n",
    "            while i < len(tokens) - 1:\n",
    "                current_pair = (tokens[i], tokens[i + 1])\n",
    "\n",
    "                # 检查是否可以合并\n",
    "                for a, b in sorted_merges:\n",
    "                    if current_pair == (a, b):\n",
    "                        # 执行合并\n",
    "                        merged = a + b\n",
    "                        tokens[i] = merged\n",
    "                        tokens.pop(i + 1)\n",
    "                        changed = True\n",
    "                        break\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        \"\"\"\n",
    "        对字符串可迭代对象进行编码，惰性生成token IDs\n",
    "\n",
    "        Args:\n",
    "            iterable: 字符串可迭代对象\n",
    "\n",
    "        Yields:\n",
    "            token IDs\n",
    "        \"\"\"\n",
    "        for text in iterable:\n",
    "            token_ids = self.encode(text)\n",
    "            for token_id in token_ids:\n",
    "                yield token_id\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        将token ID序列解码为文本\n",
    "\n",
    "        Args:\n",
    "            ids: token ID列表\n",
    "\n",
    "        Returns:\n",
    "            解码后的文本\n",
    "        \"\"\"\n",
    "        byte_sequence = b''\n",
    "        for token_id in ids:\n",
    "            if token_id in self.vocab:\n",
    "                byte_sequence += self.vocab[token_id]\n",
    "            else:\n",
    "                # 处理未知ID（使用替换字符）\n",
    "                replacement_char = b'\\xef\\xbf\\xbd'  # UTF-8替换字符\n",
    "                byte_sequence += replacement_char\n",
    "\n",
    "        try:\n",
    "            return byte_sequence.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            # 如果UTF-8解码失败，使用错误处理\n",
    "            return byte_sequence.decode('utf-8', errors='replace')\n",
    "\n",
    "\n",
    "# 测试函数\n",
    "def test_tokenizer():\n",
    "    \"\"\"测试Tokenizer类\"\"\"\n",
    "\n",
    "    # 创建测试词汇表和合并列表\n",
    "    user_dir = os.path.expanduser('~')\n",
    "    file_path = os.path.join(user_dir, 'Downloads', 'tinystories_sample.txt')\n",
    "\n",
    "    # 检查文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"测试文件大小: {file_size} 字节\")\n",
    "\n",
    "    # 并行训练\n",
    "    vocab, merges = parallel_bpe_training(\n",
    "        input_path=file_path,\n",
    "        vocab_size=10000,\n",
    "        special_tokens=['<pad>', '<unk>', '<s>', '</s>'],\n",
    "        max_merges=None,\n",
    "        num_processes=2\n",
    "    )\n",
    "    special_tokens = ['<pad>', '<unk>', '<s>', '</s>']\n",
    "    # 创建分词器\n",
    "    tokenizer = Tokenizer(vocab, merges, special_tokens)\n",
    "\n",
    "    def calculate_compression_ratio(text, tokenizer):\n",
    "        # 原始文本大小（字符数）\n",
    "        original_size = len(text)\n",
    "\n",
    "        # 编码为tokens\n",
    "        tokens = tokenizer.encode(text)\n",
    "\n",
    "        # 编码后的大小（token数）\n",
    "        encoded_size = len(tokens)\n",
    "\n",
    "        # 压缩比 = 原始大小 / 编码后大小\n",
    "        compression_ratio = original_size / encoded_size if encoded_size > 0 else 0\n",
    "\n",
    "        return original_size, encoded_size, compression_ratio\n",
    "\n",
    "    def calculate_throughput(text, tokenizer, iterations=100):\n",
    "        # 编码吞吐量\n",
    "        start_time = time.time()\n",
    "        for _ in range(iterations):\n",
    "            _ = tokenizer.encode(text)\n",
    "        encode_time = time.time() - start_time\n",
    "\n",
    "        tokens = tokenizer.encode(text)\n",
    "\n",
    "        # 解码吞吐量\n",
    "        start_time = time.time()\n",
    "        for _ in range(iterations):\n",
    "            _ = tokenizer.decode(tokens)\n",
    "        decode_time = time.time() - start_time\n",
    "\n",
    "        # 计算每秒处理的字符数\n",
    "        encode_throughput = (len(text) * iterations) / encode_time\n",
    "        decode_throughput = (len(text) * iterations) / decode_time\n",
    "\n",
    "        # 计算每秒处理的token数\n",
    "        encode_token_throughput = (len(tokens) * iterations) / encode_time\n",
    "        decode_token_throughput = (len(tokens) * iterations) / decode_time\n",
    "\n",
    "        return (encode_throughput, decode_throughput,\n",
    "                encode_token_throughput, decode_token_throughput)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        file_content = f.read()\n",
    "    #对文本进行编码和解码\n",
    "    encoded_tokens = tokenizer.encode(file_content)\n",
    "    decoded_text = tokenizer.decode(encoded_tokens)\n",
    "\n",
    "    #计算压缩比\n",
    "    original_size, encoded_size, compression_ratio = calculate_compression_ratio(file_content, tokenizer)\n",
    "\n",
    "    #计算吞吐量\n",
    "    encode_throughput, decode_throughput, encode_token_throughput, decode_token_throughput = calculate_throughput(\n",
    "        file_content[:10000], tokenizer)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Tokenizer 性能测试结果\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(f\"原始文本大小: {original_size} 字符\")\n",
    "    print(f\"编码后token数量: {encoded_size} tokens\")\n",
    "    print(f\"压缩比: {compression_ratio:.2f} (字符/token)\")\n",
    "\n",
    "    print(\"\\n吞吐量测试:\")\n",
    "    print(f\"编码吞吐量: {encode_throughput:.2f} 字符/秒\")\n",
    "    print(f\"解码吞吐量: {decode_throughput:.2f} 字符/秒\")\n",
    "    print(f\"编码吞吐量: {encode_token_throughput:.2f} tokens/秒\")\n",
    "    print(f\"解码吞吐量: {decode_token_throughput:.2f} tokens/秒\")\n",
    "\n",
    "    # 测试文件保存和加载（模拟）\n",
    "    #print(\"\\nTesting file I/O simulation...\")\n",
    "\n",
    "    # 创建另一个分词器测试encode_iterable\n",
    "    #simple_vocab = {i: bytes([i]) for i in range(256)}\n",
    "    #simple_vocab[256] = b'test'\n",
    "    #simple_merges = []\n",
    "    #simple_tokenizer = Tokenizer(simple_vocab, simple_merges)\n",
    "\n",
    "    # 测试encode_iterable\n",
    "    #texts = [\"hello\", \"world\", \"test\"]\n",
    "    #print(\"Testing encode_iterable:\")\n",
    "    #for token_id in simple_tokenizer.encode_iterable(texts):\n",
    "    #    print(f\"  Yielded token ID: {token_id}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ec4b7-e9e0-4058-8c9d-5cc5db39b329",
   "metadata": {},
   "source": [
    "结果：原始文本大小: 3786 字符\n",
    "编码后token数量: 1881 tokens\n",
    "压缩比: 2.01 (字符/token)\n",
    "\n",
    "吞吐量测试:\n",
    "编码吞吐量: 9363.55 字符/秒\n",
    "解码吞吐量: 14811296.15 字符/秒\n",
    "编码吞吐量: 4652.10 tokens/秒\n",
    "解码吞吐量: 7358702.61 tokens/秒\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46158c4-68c5-4520-b2df-9a6602d1afa3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19804da7-7a16-42a9-a95f-b58264ee394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, device=None, dtype=None):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            in_features: int - 输入的最终维度\n",
    "            out_features: int - 输出的最终维度  \n",
    "            device: torch.device | None = None - 存储参数的设备\n",
    "            dtype: torch.dtype | None = None - 参数的数据类型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 调用父类构造函数\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        # 存储维度信息\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # 参数设置\n",
    "        factory_kwargs = {}\n",
    "        if device is not None:\n",
    "            factory_kwargs['device'] = device\n",
    "        if dtype is not None:\n",
    "            factory_kwargs['dtype'] = dtype\n",
    "        \n",
    "        # 创建权重参数 W (形状: out_features × in_features)\n",
    "        self.weight = nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        \n",
    "        # 使用截断正态分布初始化权重\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"使用截断正态分布初始化权重参数\"\"\"\n",
    "        init.trunc_normal_(self.weight, mean=0.0, std=1.0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        对输入应用线性变换\n",
    "        \n",
    "        参数:\n",
    "            x: torch.Tensor - 输入张量，形状为 (..., in_features)\n",
    "            \n",
    "        返回:\n",
    "            torch.Tensor - 输出张量，形状为 (..., out_features)\n",
    "        \"\"\"\n",
    "        # 执行线性变换: output = x @ W^T\n",
    "        # 因为我们的 W 形状是 (out_features, in_features)\n",
    "        # 所以需要转置为 (in_features, out_features) 来进行矩阵乘法\n",
    "        return x @ self.weight.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eaec27-1bc1-4bd2-bb21-fc65ece31f1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a532c658-c1ff-45fc-b836-84dc6e563613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 初始化权重矩阵\n",
    "        self.weight = nn.Parameter(torch.empty((num_embeddings, embedding_dim), \n",
    "                                              device=device, dtype=dtype))\n",
    "        \n",
    "        # 使用截断正态分布初始化权重\n",
    "        init.trunc_normal_(self.weight, mean=0.0, std=1.0)\n",
    "    \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # 根据 token_ids 查找对应的嵌入向量\n",
    "        return self.weight[token_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf027d-e691-488e-a755-744da974af81",
   "metadata": {},
   "source": [
    "## RMSnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36697b79-ffa6-4228-bfea-717a292efee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "        \n",
    "        # 初始化可学习的缩放参数 gamma\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model, **factory_kwargs))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 保存原始数据类型以便之后下转换\n",
    "        original_dtype = x.dtype\n",
    "        \n",
    "        # 上转换为 float32 进行归一化计算\n",
    "        x_float = x.float()\n",
    "        \n",
    "        # 计算均方根 (RMS)\n",
    "        # 沿最后一个维度 (d_model) 计算，保持维度以便广播\n",
    "        rms = torch.sqrt(torch.mean(x_float ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        \n",
    "        # 应用 RMS 归一化\n",
    "        x_normalized = x_float / rms\n",
    "        \n",
    "        # 应用缩放参数 gamma\n",
    "        x_normalized = x_normalized * self.gamma\n",
    "        \n",
    "        # 下转换回原始数据类型\n",
    "        return x_normalized.to(original_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb53d1-f83a-4ea5-af57-2432d83dee57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd397c-1081-4651-a2f6-1ec2e7e627c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def run_softmax(tensor, dim):\n",
    "    \"\"\"\n",
    "    对输入张量沿指定维度应用 softmax 操作。\n",
    "\n",
    "    参数:\n",
    "        tensor (torch.Tensor): 输入张量。\n",
    "        dim (int): 应用 softmax 的维度。\n",
    "\n",
    "    返回:\n",
    "        torch.Tensor: 输出张量，形状与输入相同，指定维度为概率分布。\n",
    "    \"\"\"\n",
    "    # 减去指定维度的最大值以提高数值稳定性\n",
    "    max_vals = torch.max(tensor, dim=dim, keepdim=True).values\n",
    "    shifted = tensor - max_vals\n",
    "    exp_tensor = torch.exp(shifted)\n",
    "     sum_exp= torch.sum(exp_tensor, dim=dim, keepdim=True)\n",
    "    softmax_tensor = exp_tensor / sum_exp\n",
    "    return softmax_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f91b9e7-0825-4aed-b43b-613a501667b7",
   "metadata": {},
   "source": [
    "减去最大值的原因：防止原始值过大时exp（x）超出浮点数表示范围\n",
    "\n",
    "保持维度 指的是在张量操作（如求和、求最大值等）后，保持原始张量的维度结构，而不是压缩减少的维度。\n",
    "假设我们有一个 2×3 的张量：\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "情况1：不保持维度 (keepdim=False，默认值)\n",
    "max_vals = torch.max(tensor, dim=1)  # 沿行维度求最大值\n",
    "print(max_vals.values)  # 输出: tensor([3, 6])\n",
    "print(max_vals.values.shape)  # 输出: torch.Size([2])\n",
    "\n",
    "结果从 2×3 变成了 1维张量 [3, 6]，维度从 2 维变成了 1 维。\n",
    "\n",
    "情况2：保持维度 (keepdim=True)\n",
    "\n",
    "max_vals = torch.max(tensor, dim=1, keepdim=True)\n",
    "print(max_vals.values)  # 输出: tensor([[3], [6]])\n",
    "print(max_vals.values.shape)  # 输出: torch.Size([2, 1])\n",
    "\n",
    "结果保持为 2×1 的张量，仍然是 2 维。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643911b9-79e6-4dd8-8ab5-fb90054ae4c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## positionwise_feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be05de6-5188-4fbb-8516-a38a385fea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SwiGLUFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 计算中间维度，确保是64的倍数\n",
    "        d_ff_raw = (8/3) * d_model\n",
    "        d_ff = int(round(d_ff_raw / 64)) * 64\n",
    "        \n",
    "        # 第一个线性层扩展到2倍维度（用于GLU拆分）\n",
    "        self.linear1 = nn.Linear(d_model, 2 * d_ff)\n",
    "        \n",
    "        # 第二个线性层投影回原始维度\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 第一个线性变换\n",
    "        x_gate = self.linear1(x)\n",
    "        \n",
    "        # 拆分为两部分用于GLU\n",
    "        x1, x2 = x_gate.chunk(2, dim=-1)\n",
    "        \n",
    "        # 手动实现SiLU: x * sigmoid(x)\n",
    "        swish = x1 * torch.sigmoid(x1)\n",
    "        \n",
    "        # GLU门控部分\n",
    "        gate = torch.sigmoid(x2)\n",
    "        \n",
    "        # GLU操作：逐元素相乘\n",
    "        glu_output = swish * gate\n",
    "        \n",
    "        # 应用dropout\n",
    "        glu_output = self.dropout(glu_output)\n",
    "        \n",
    "        # 第二个线性变换投影回原始维度\n",
    "        output = self.linear2(glu_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 测试适配器函数\n",
    "def run_swiglu(input_tensor, d_model, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_tensor: 输入张量\n",
    "        d_model: 模型维度\n",
    "        dropout: dropout率\n",
    "    Returns:\n",
    "        output_tensor: 输出张量\n",
    "    \"\"\"\n",
    "    swiglu = SwiGLUFeedForward(d_model, dropout)\n",
    "    return swiglu(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af4a9c-80db-41a1-9d24-4421dcfb6af0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8ed99-7191-44e7-aed6-068be3e7aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):\n",
    "        super().__init__()\n",
    "        self.theta = theta\n",
    "        self.d_k = d_k\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # 预计算正弦和余弦值\n",
    "        positions = torch.arange(max_seq_len, device=device).float()\n",
    "        indices = torch.arange(0, d_k, 2, device=device).float()\n",
    "        \n",
    "        # 计算角度：theta^(-2i/d_k) * pos\n",
    "        theta_powered = theta ** (-indices / d_k)\n",
    "        angles = positions.unsqueeze(1) * theta_powered.unsqueeze(0)  # (max_seq_len, d_k/2)\n",
    "        \n",
    "        # 计算正弦和余弦\n",
    "        cos_vals = torch.cos(angles)  # (max_seq_len, d_k/2)\n",
    "        sin_vals = torch.sin(angles)  # (max_seq_len, d_k/2)\n",
    "        \n",
    "        # 注册为缓冲区，这样它们会随模型一起移动设备且不需要梯度\n",
    "        self.register_buffer('cos_cached', cos_vals, persistent=False)\n",
    "        self.register_buffer('sin_cached', sin_vals, persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        应用旋转位置编码到输入张量\n",
    "        \n",
    "        Args:\n",
    "            x: 形状为 (..., seq_len, d_k) 的输入张量\n",
    "            token_positions: 形状为 (..., seq_len) 的标记位置张量\n",
    "            \n",
    "        Returns:\n",
    "            应用RoPE后的张量，形状与x相同\n",
    "        \"\"\"\n",
    "        batch_dims = x.shape[:-2]  # 获取批次维度\n",
    "        seq_len = x.shape[-2]\n",
    "        \n",
    "        # 重塑x以分离实部和虚部（相邻维度为一对）\n",
    "        x_reshaped = x.reshape(*batch_dims, seq_len, self.d_k // 2, 2)\n",
    "        \n",
    "        # 根据token_positions获取对应的cos和sin值\n",
    "        # token_positions形状: (..., seq_len) -> 展平以进行索引\n",
    "        flat_positions = token_positions.reshape(-1)\n",
    "        \n",
    "        # 索引预先计算的cos和sin值\n",
    "        cos_vals = self.cos_cached[flat_positions]  # (total_tokens, d_k/2)\n",
    "        sin_vals = self.sin_cached[flat_positions]  # (total_tokens, d_k/2)\n",
    "        \n",
    "        # 重塑回原始批次形状加上序列和特征维度\n",
    "        cos_vals = cos_vals.reshape(*batch_dims, seq_len, self.d_k // 2, 1)\n",
    "        sin_vals = sin_vals.reshape(*batch_dims, seq_len, self.d_k // 2, 1)\n",
    "        \n",
    "        # 应用旋转：对于每对(x_i, x_{i+1})，应用2D旋转\n",
    "        x_rotated = torch.stack([\n",
    "            x_reshaped[..., 0] * cos_vals.squeeze(-1) - x_reshaped[..., 1] * sin_vals.squeeze(-1),\n",
    "            x_reshaped[..., 0] * sin_vals.squeeze(-1) + x_reshaped[..., 1] * cos_vals.squeeze(-1)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # 重塑回原始形状\n",
    "        return x_rotated.reshape(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89a3e1-0c86-4d14-81e1-4b7b2eab4c2a",
   "metadata": {},
   "source": [
    "张量x的形状：(batch_size, ..., seq_len, d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a60e1-99f6-4318-a78c-e21f4a816d66",
   "metadata": {},
   "source": [
    "## scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284dc3e5-1ac0-4d56-bf07-d68dcdb3086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "        query: 形状为 (batch_size, ..., seq_len_q, d_k) 的张量\n",
    "        key: 形状为 (batch_size, ..., seq_len_k, d_k) 的张量\n",
    "        value: 形状为 (batch_size, ..., seq_len_v, d_v) 的张量 (通常 seq_len_k = seq_len_v)\n",
    "        mask: 形状为 (seq_len_q, seq_len_k) 的布尔张量 (可选)\n",
    "\n",
    "    返回:\n",
    "        输出: 形状为 (batch_size, ..., seq_len_q, d_v) 的张量\n",
    "    \"\"\"\n",
    "    # 获取 key 的最后一个维度 d_k\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # 计算点积并缩放\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    \n",
    "    # 如果提供了掩码，应用掩码\n",
    "    if mask is not None:\n",
    "        # 将掩码中 False 的位置对应的分数设置为一个非常大的负值\n",
    "        scores = scores.masked_fill(mask == False, -1e9)\n",
    "    \n",
    "    # 在最后一个维度（key 的序列维度）上应用 softmax 来获取注意力权重\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 使用注意力权重对 value 进行加权求和\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c19497-8a48-43f5-b7a3-e0846a0c0c2b",
   "metadata": {},
   "source": [
    "## multihead_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae18737-7d70-48cb-bb51-5f182cdbf11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
